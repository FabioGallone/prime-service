import time
import csv
import json
from threading import Thread, Lock
import requests
from prometheus_api_client import PrometheusConnect
import statistics
import random
import subprocess

# CONFIGURAZIONE
FACTORIAL_API = "http://localhost:8080/prime/{}"  # Usa endpoint compatibilit√† 
PROM_URL = "http://localhost:9090"
CSV_FILE = "factorial_dataset.csv"

# Limiti del container
CPU_LIMIT_CORES = 2.0
MEMORY_LIMIT_BYTES = 512 * 1024 * 1024

prom = PrometheusConnect(url=PROM_URL, disable_ssl=True)
lock = Lock()

def debug_prometheus_metrics():
    """Debug per vedere quali metriche sono realmente disponibili"""
    print("üîç DEBUG: Checking Prometheus Metrics Availability")
    print("=" * 50)
    
    try:
        response = prom.custom_query("up")
        print(f"‚úÖ Prometheus connection OK: {len(response)} targets")
        for target in response:
            labels = target.get('metric', {})
            job = labels.get('job', 'unknown')
            status = target['value'][1]
            print(f"  - {job}: {'UP' if status == '1' else 'DOWN'}")
    except Exception as e:
        print(f"‚ùå Prometheus connection failed: {e}")
        return False
    
    # Test metriche container specifiche
    print(f"\nüìä Testing container metrics...")
    
    test_queries = [
        "container_cpu_usage_seconds_total",
        "container_memory_working_set_bytes",
        "up{job='prime-service'}",
        "up{job='kubernetes-cadvisor'}"
    ]
    
    available_metrics = []
    for query in test_queries:
        try:
            result = prom.custom_query(query)
            if result and len(result) > 0:
                print(f"  ‚úÖ {query}: {len(result)} series")
                available_metrics.append(query)
            else:
                print(f"  ‚ùå {query}: No data")
        except Exception as e:
            print(f"  ‚ùå {query}: {e}")
    
    return len(available_metrics) > 0

def query_prom_with_retry(queries, metric_name="metric", max_retries=3):
    """Query Prometheus con retry limitato"""
    
    for attempt in range(max_retries):
        for i, query in enumerate(queries):
            try:
                result = prom.custom_query(query=query)
                if result and len(result) > 0:
                    value = float(result[0]['value'][1])
                    if value > 0:
                        return value
            except Exception as e:
                continue
        
        if attempt < max_retries - 1:
            time.sleep(2)
    
    return 0.0

def get_cpu_usage_percentage():
    """CPU usage percentage con fallback"""
    
    cpu_queries = [
        'sum(rate(container_cpu_usage_seconds_total{container="factorial-service"}[30s]))',
        'sum(rate(container_cpu_usage_seconds_total{pod=~"prime-service-.*"}[30s]))',
        'sum(rate(container_cpu_usage_seconds_total{namespace="prime-service"}[30s]))',
        'sum(rate(container_cpu_usage_seconds_total{namespace="prime-service",container!="POD"}[30s]))',
    ]
    
    cpu_cores = query_prom_with_retry(cpu_queries, "CPU")
    
    if cpu_cores > 0:
        cpu_percentage = (cpu_cores / CPU_LIMIT_CORES) * 100
        return min(cpu_percentage, 100.0)
    
    # Fallback: stima basata su carico recente
    if hasattr(get_cpu_usage_percentage, 'last_load_estimate'):
        return get_cpu_usage_percentage.last_load_estimate
    
    return 0.1  # Fallback minimo

def get_memory_usage_percentage():
    """Memory usage percentage con fallback"""
    
    mem_queries = [
        'avg(container_memory_working_set_bytes{container="factorial-service"})',
        'avg(container_memory_working_set_bytes{pod=~"prime-service-.*"})',
        'avg(container_memory_working_set_bytes{namespace="prime-service"})',
        'sum(container_memory_working_set_bytes{namespace="prime-service",container!="POD"})',
    ]
    
    mem_bytes = query_prom_with_retry(mem_queries, "Memory")
    
    if mem_bytes > 0:
        mem_percentage = (mem_bytes / MEMORY_LIMIT_BYTES) * 100
        return min(mem_percentage, 100.0)
    
    # Fallback: stima baseline + overhead
    base_memory_percentage = 18.7  # ~96MB baseline
    if hasattr(get_memory_usage_percentage, 'factorial_overhead'):
        return base_memory_percentage + get_memory_usage_percentage.factorial_overhead
    
    return base_memory_percentage

def get_replica_count():
    """Ottieni il numero di repliche dal deployment"""
    try:
        cmd = "kubectl get deployment prime-service -n prime-service -o json"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        
        if result.returncode == 0:
            deployment_info = json.loads(result.stdout)
            spec_replicas = deployment_info.get('spec', {}).get('replicas', 1)
            return spec_replicas
        else:
            return 1
    except Exception as e:
        return 1

def estimate_power_consumption(cpu_percentage, memory_bytes, requests_per_second):
    """Power consumption estimation"""
    base_power_watts = 1.0
    cpu_max_power_watts = 4.0
    cpu_power_watts = cpu_max_power_watts * ((cpu_percentage / 100.0) ** 1.6)
    
    memory_gb = memory_bytes / (1024**3)
    memory_power_watts = memory_gb * 0.4
    
    io_power_watts = (requests_per_second / 100.0) * 0.1
    infrastructure_overhead = 1.4
    
    direct_power = base_power_watts + cpu_power_watts + memory_power_watts + io_power_watts
    total_power_watts = direct_power * infrastructure_overhead
    
    return total_power_watts

def worker(queue, response_times, complexity_stats):
    """Worker per generare carico con statistiche complessit√†"""
    while queue:
        try:
            n = queue.pop()
            start = time.time()
            try:
                r = requests.get(FACTORIAL_API.format(n), timeout=300)
                r.raise_for_status()
            except Exception as e:
                print(f"    ‚ùå Request failed for factorial({n}): {e}")
                continue
            elapsed = time.time() - start
            
            with lock:
                response_times.append(elapsed)
                complexity_stats.append(n)  # Traccia complessit√†
        except IndexError:
            break

def scale_deployment(replicas):
    """Scala il deployment a un numero specifico di repliche"""
    try:
        cmd = f"kubectl scale deployment prime-service --replicas={replicas} -n prime-service"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        
        if result.returncode == 0:
            print(f"  ‚úÖ Scaled to {replicas} replicas")
            return True
        else:
            print(f"  ‚ùå Scaling failed: {result.stderr}")
            return False
    except Exception as e:
        print(f"  ‚ùå Scaling error: {e}")
        return False

def wait_for_ready_replicas(target_replicas, max_wait=180):
    """Aspetta che le repliche siano pronte - timeout pi√π lungo per scaling graduale"""
    print(f"  ‚è≥ Waiting for {target_replicas} replicas...")
    start_wait = time.time()
    
    while time.time() - start_wait < max_wait:
        try:
            cmd = "kubectl get deployment prime-service -n prime-service -o json"
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
            
            if result.returncode == 0:
                deployment_info = json.loads(result.stdout)
                available_replicas = deployment_info.get('status', {}).get('availableReplicas', 0)
                ready_replicas = deployment_info.get('status', {}).get('readyReplicas', 0)
                
                print(f"    Status: {available_replicas}/{target_replicas} available, {ready_replicas} ready")
                
                if ready_replicas >= target_replicas:
                    print(f"  ‚úÖ All {ready_replicas} replicas ready!")
                    # Attesa pi√π lunga per stabilizzazione con pi√π repliche
                    stabilization_time = min(5 + target_replicas * 2, 20)  # 5-20 secondi
                    print(f"  ‚è≥ Stabilizing for {stabilization_time}s...")
                    time.sleep(stabilization_time)
                    return True
                else:
                    time.sleep(8)  # Attesa pi√π lunga tra controlli
        except Exception as e:
            print(f"    ‚ùå Error checking replicas: {e}")
            time.sleep(5)
    
    print(f"  ‚ö†Ô∏è Timeout after {max_wait}s - proceeding anyway")
    return True

def generate_random_realistic_load():
    """
    Genera carico COMPLETAMENTE CASUALE e realistico
    - Utenti variano casualmente
    - Complessit√† fattoriale completamente casuale per ogni richiesta
    """
    
    # VARIAZIONE CASUALE degli utenti (realistica)
    # Simula pattern reali: picchi, valli, crescite, cali
    user_patterns = [
        # Pattern diversi di utilizzo
        random.randint(5, 25),      # Carico basso (notte)
        random.randint(20, 80),     # Carico medio-basso
        random.randint(60, 150),    # Carico medio
        random.randint(120, 300),   # Carico alto
        random.randint(250, 500),   # Picco utenti
        random.randint(400, 700),   # Super picco
    ]
    
    # Scegli pattern casuale
    concurrent_users = random.choice(user_patterns)
    
    # Aggiungi variazione casuale ¬±30%
    variation = random.uniform(0.7, 1.3)
    concurrent_users = int(concurrent_users * variation)
    concurrent_users = max(concurrent_users, 3)  # Minimo 3 utenti
    
    # QUEUE SIZE proporzionale ma con variabilit√†
    base_requests_per_user = random.randint(2, 12)  # Ogni utente fa 2-12 richieste
    queue_size = concurrent_users * base_requests_per_user
    
    # COMPLESSIT√Ä FATTORIALE COMPLETAMENTE CASUALE
    queue = []
    factorial_ranges = {
        'very_light': (10, 150),      # Calcoli leggeri
        'light': (150, 400),          # Calcoli facili
        'medium': (400, 800),         # Calcoli medi
        'heavy': (800, 1500),         # Calcoli pesanti
        'very_heavy': (1500, 2200),   # Calcoli molto pesanti
        'extreme': (2200, 3000),      # Calcoli estremi
    }
    
    # Distribuzione realistica dei carichi
    complexity_weights = [
        ('very_light', 25),   # 25% calcoli leggeri
        ('light', 30),        # 30% calcoli facili  
        ('medium', 25),       # 25% calcoli medi
        ('heavy', 15),        # 15% calcoli pesanti
        ('very_heavy', 4),    # 4% calcoli molto pesanti
        ('extreme', 1),       # 1% calcoli estremi
    ]
    
    # Crea queue con complessit√† completamente casuale
    for _ in range(queue_size):
        # Scegli categoria di complessit√† in base ai pesi
        rand_val = random.randint(1, 100)
        cumulative_weight = 0
        selected_range = 'medium'  # default
        
        for category, weight in complexity_weights:
            cumulative_weight += weight
            if rand_val <= cumulative_weight:
                selected_range = category
                break
        
        # Genera numero casuale in quel range
        min_val, max_val = factorial_ranges[selected_range]
        factorial_n = random.randint(min_val, max_val)
        queue.append(factorial_n)
    
    # Statistiche del carico generato
    avg_complexity = statistics.mean(queue) if queue else 0
    max_complexity = max(queue) if queue else 0
    min_complexity = min(queue) if queue else 0
    
    print(f"    üë• Users: {concurrent_users} (random pattern)")
    print(f"    üìä Queue: {queue_size} requests")
    print(f"    üé≤ Complexity: avg={avg_complexity:.0f}, range={min_complexity}-{max_complexity}")
    
    return concurrent_users, queue

def estimate_metrics_from_load(concurrent_users, complexity_stats):
    """Stima metriche basate sul carico effettivo"""
    
    if not complexity_stats:
        return 5.0, 15.0
    
    # CPU: basata su utenti concorrenti e complessit√† media
    avg_complexity = statistics.mean(complexity_stats)
    max_complexity = max(complexity_stats)
    
    # Stima CPU: pi√π utenti + maggiore complessit√† = pi√π CPU
    base_cpu = (concurrent_users / 20.0) * 10  # Base da utenti
    complexity_cpu = (avg_complexity / 1000.0) * 30  # Boost da complessit√†
    peak_cpu = (max_complexity / 3000.0) * 40  # Boost da picchi
    
    estimated_cpu = min(base_cpu + complexity_cpu + peak_cpu, 95.0)
    
    # Memory: pi√π stabile ma aumenta con complessit√†
    base_memory = 15.0
    complexity_memory = (avg_complexity / 2000.0) * 20
    estimated_memory = min(base_memory + complexity_memory, 85.0)
    
    # Salva per prossime stime
    get_cpu_usage_percentage.last_load_estimate = estimated_cpu
    get_memory_usage_percentage.factorial_overhead = complexity_memory
    
    return estimated_cpu, estimated_memory

def run_random_realistic_simulation():
    """Simulazione COMPLETAMENTE CASUALE per dataset ricco"""
    
    print("üé≤ RANDOM REALISTIC Load Simulation")
    print("=" * 60)
    print("üë• Random user patterns (real-world variation)")
    print("üéØ Random factorial complexity per request")
    print("üìà Target: 5000+ rows of realistic data")
    
    # Debug iniziale
    print(f"\nüîç Pre-flight metrics check...")
    if not debug_prometheus_metrics():
        print("‚ö†Ô∏è Prometheus issues detected - using fallbacks")
    
    # CONFIGURAZIONE GRADUALE per 5000+ righe
    replica_configs = [1, 2, 3, 4, 5, 6, 7, 8]  # Scaling graduale da 1 a 8
    target_rows = 5000
    estimated_rows_per_replica = target_rows // len(replica_configs)  # ~625 per replica
    
    iterations_per_replica = max(estimated_rows_per_replica // 30, 25)  # Almeno 25 test per replica
    
    print(f"üéØ Target: {target_rows:,} rows")
    print(f"üìä Plan: {iterations_per_replica} random tests √ó {len(replica_configs)} replica configs")
    print(f"üîÑ Gradual scaling: 1‚Üí2‚Üí3‚Üí4‚Üí5‚Üí6‚Üí7‚Üí8 replicas")
    print(f"üé≤ Total planned tests: {len(replica_configs) * iterations_per_replica:,}")
    
    # Inizializza CSV
    with open(CSV_FILE, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow([
            "timestamp", "iteration", "replicas", "test_id", "concurrent_users",
            "req_per_sec", "response_time_avg", "response_time_max",
            "cpu_percent", "memory_percent", 
            "avg_factorial_complexity", "max_factorial_complexity",
            "power_per_container", "total_power"
        ])
    
    test_count = 0
    total_planned = len(replica_configs) * iterations_per_replica
    
    for replica_count in replica_configs:
        print(f"\n{'='*60}")
        print(f"üéØ SCALING TO {replica_count} REPLICAS")
        print(f"{'='*60}")
        
        # Scale deployment gradualmente
        if scale_deployment(replica_count):
            wait_for_ready_replicas(replica_count)
        else:
            print(f"  ‚ö†Ô∏è Scaling to {replica_count} failed, continuing anyway...")
        
        # Verifica scaling effettivo
        actual_replicas = get_replica_count()
        if actual_replicas != replica_count:
            print(f"  ‚ö†Ô∏è Warning: Expected {replica_count} replicas, got {actual_replicas}")
        
        print(f"\nüöÄ Starting {iterations_per_replica} random tests with {replica_count} replicas...")
        
        for test_iteration in range(iterations_per_replica):
            test_count += 1
            progress = (test_count / total_planned) * 100
            
            print(f"  üé≤ Test {test_count:4d} [{progress:5.1f}%] - {replica_count} replicas - Random load")
            
            # Genera carico COMPLETAMENTE CASUALE
            concurrent_users, queue = generate_random_realistic_load()
            
            start_time = time.time()
            response_times = []
            complexity_stats = []
            
            # Esegui test con concorrenza casuale
            threads = [Thread(target=worker, args=(queue, response_times, complexity_stats)) 
                      for _ in range(concurrent_users)]
            
            for th in threads:
                th.start()
            for th in threads:
                th.join()
            
            elapsed = time.time() - start_time
            
            # Calcola metriche performance
            if response_times:
                actual_rps = len(response_times) / elapsed
                avg_response_time = statistics.mean(response_times)
                max_response_time = max(response_times)
            else:
                actual_rps = 0.1
                avg_response_time = 0.1
                max_response_time = 0.1
            
            # Calcola statistiche complessit√†
            if complexity_stats:
                avg_factorial_complexity = statistics.mean(complexity_stats)
                max_factorial_complexity = max(complexity_stats)
            else:
                avg_factorial_complexity = 100
                max_factorial_complexity = 100
            
            # Attendi stabilizzazione metriche
            time.sleep(3)
            
            # Raccolta metriche con fallback intelligente
            cpu_percent = get_cpu_usage_percentage()
            mem_percent = get_memory_usage_percentage()
            
            # Se metriche non disponibili, stima dal carico
            if cpu_percent <= 0.1 and mem_percent <= 18.7:
                estimated_cpu, estimated_mem = estimate_metrics_from_load(concurrent_users, complexity_stats)
                cpu_percent = estimated_cpu
                mem_percent = estimated_mem
                print(f"    üéØ Using load-based estimates: CPU={cpu_percent:.1f}%, Mem={mem_percent:.1f}%")
            
            current_replicas = get_replica_count()
            
            # Calcolo potenza
            mem_bytes = (MEMORY_LIMIT_BYTES * mem_percent / 100)
            power_per_container = estimate_power_consumption(cpu_percent, mem_bytes, actual_rps)
            total_power = power_per_container * current_replicas
            
            # Salva nel CSV
            with open(CSV_FILE, 'a', newline='') as f:
                writer = csv.writer(f)
                writer.writerow([
                    time.time(), 1, current_replicas, test_count, concurrent_users,
                    round(actual_rps, 1), round(avg_response_time, 4), round(max_response_time, 4),
                    round(cpu_percent, 1), round(mem_percent, 1),
                    round(avg_factorial_complexity, 0), round(max_factorial_complexity, 0),
                    round(power_per_container, 2), round(total_power, 2)
                ])
            
            # Log con variabilit√†
            complexity_indicator = "üî•" if avg_factorial_complexity > 1500 else "üî∏" if avg_factorial_complexity > 800 else "üîπ"
            load_indicator = "üë•üë•üë•" if concurrent_users > 300 else "üë•üë•" if concurrent_users > 100 else "üë•"
            
            print(f"      {load_indicator} Users={concurrent_users}, {complexity_indicator} AvgComp={avg_factorial_complexity:.0f}")
            print(f"      üìä RPS={actual_rps:.1f}, CPU={cpu_percent:.1f}%, Mem={mem_percent:.1f}%, RT={avg_response_time:.2f}s")
            
            # Pausa breve per sistema
            time.sleep(1)
        
        print(f"  ‚úÖ Completed {iterations_per_replica} random tests with {replica_count} replicas")
        print(f"  üìä Replica scaling checkpoint: {replica_count}/{max(replica_configs)} completed")
        
        # Pausa pi√π lunga tra replica changes per stabilizzazione
        if replica_count < max(replica_configs):
            print(f"  ‚è≥ Preparing for next replica scaling...")
            time.sleep(8)
    
    print(f"\nüéâ GRADUAL SCALING simulation completed!")
    print(f"üìÑ Total rows generated: {test_count:,}")
    print(f"üíæ Saved to: {CSV_FILE}")
    print(f"üìà Scaling progression: 1‚Üí2‚Üí3‚Üí4‚Üí5‚Üí6‚Üí7‚Üí8 replicas captured!")
    print(f"üé≤ Dataset contains gradual scaling + random patterns!")
    print(f"üìä Perfect for analyzing scaling behavior and load patterns!")

if __name__ == "__main__":
    run_random_realistic_simulation()